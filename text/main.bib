
@article{mahler_dex-net_2017,
	title = {Dex-{Net} 2.0: {Deep} {Learning} to {Plan} {Robust} {Grasps} with {Synthetic} {Point} {Clouds} and {Analytic} {Grasp} {Metrics}},
	shorttitle = {Dex-{Net} 2.0},
	url = {http://arxiv.org/abs/1703.09312},
	abstract = {To reduce data collection time for deep learning of robust robotic grasp plans, we explore training from a synthetic dataset of 6.7 million point clouds, grasps, and analytic grasp metrics generated from thousands of 3D models from Dex-Net 1.0 in randomized poses on a table. We use the resulting dataset, DexNet 2.0, to train a Grasp Quality Convolutional Neural Network (GQ-CNN) model that rapidly predicts the probability of success of grasps from depth images, where grasps are speciﬁed as the planar position, angle, and depth of a gripper relative to an RGB-D sensor. Experiments with over 1,000 trials on an ABB YuMi comparing grasp planning methods on singulated objects suggest that a GQ-CNN trained with only synthetic data from Dex-Net 2.0 can be used to plan grasps in 0.8s with a success rate of 93\% on eight known objects with adversarial geometry and is 3× faster than registering point clouds to a precomputed dataset of objects and indexing grasps. The Dex-Net 2.0 grasp planner also has the highest success rate on a dataset of 10 novel rigid objects and achieves 99\% precision (one false positive out of 69 grasps classiﬁed as robust) on a dataset of 40 novel household objects, some of which are articulated or deformable. Code, datasets, videos, and supplementary material are available at http://berkeleyautomation.github.io/dex-net.},
	language = {en},
	urldate = {2019-05-13},
	journal = {arXiv:1703.09312 [cs]},
	author = {Mahler, Jeffrey and Liang, Jacky and Niyaz, Sherdil and Laskey, Michael and Doan, Richard and Liu, Xinyu and Ojea, Juan Aparicio and Goldberg, Ken},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.09312},
	keywords = {Computer Science - Robotics},
	annote = {Comment: To appear at Robotics: Science and Systems 2017},
	file = {Mahler et al. - 2017 - Dex-Net 2.0 Deep Learning to Plan Robust Grasps w.pdf:/Users/marc/Zotero/storage/TABWSHQX/Mahler et al. - 2017 - Dex-Net 2.0 Deep Learning to Plan Robust Grasps w.pdf:application/pdf}
}

@inproceedings{mahler_dex-net_2016,
	address = {Stockholm, Sweden},
	title = {Dex-{Net} 1.0: {A} cloud-based network of 3D objects for robust grasp planning using a {Multi}-{Armed} {Bandit} model with correlated rewards},
	isbn = {978-1-4673-8026-3},
	shorttitle = {Dex-{Net} 1.0},
	url = {http://ieeexplore.ieee.org/document/7487342/},
	doi = {10.1109/ICRA.2016.7487342},
	abstract = {This paper presents the Dexterity Network (DexNet) 1.0, a dataset of 3D object models and a sampling-based planning algorithm to explore how Cloud Robotics can be used for robust grasp planning. The algorithm uses a MultiArmed Bandit model with correlated rewards to leverage prior grasps and 3D object models in a growing dataset that currently includes over 10,000 unique 3D object models and 2.5 million parallel-jaw grasps. Each grasp includes an estimate of the probability of force closure under uncertainty in object and gripper pose and friction. Dex-Net 1.0 uses Multi-View Convolutional Neural Networks (MV-CNNs), a new deep learning method for 3D object classiﬁcation, to provide a similarity metric between objects, and the Google Cloud Platform to simultaneously run up to 1,500 virtual cores, reducing experiment runtime by up to three orders of magnitude. Experiments suggest that correlated bandit techniques can use a cloud-based network of object models to signiﬁcantly reduce the number of samples required for robust grasp planning. We report on system sensitivity to variations in similarity metrics and in uncertainty in pose and friction. Code and updated information is available at http://berkeleyautomation.github.io/dex-net/.},
	language = {en},
	urldate = {2019-05-13},
	booktitle = {2016 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Mahler, Jeffrey and Pokorny, Florian T. and Hou, Brian and Roderick, Melrose and Laskey, Michael and Aubry, Mathieu and Kohlhoff, Kai and Kroger, Torsten and Kuffner, James and Goldberg, Ken},
	month = may,
	year = {2016},
	pages = {1957--1964},
	file = {Mahler et al. - 2016 - Dex-Net 1.0 A cloud-based network of 3D objects f.pdf:/Users/marc/Zotero/storage/BD3STM2J/Mahler et al. - 2016 - Dex-Net 1.0 A cloud-based network of 3D objects f.pdf:application/pdf}
}

@article{kehoe_survey_2015,
	title = {A {Survey} of {Research} on {Cloud} {Robotics} and {Automation}},
	volume = {12},
	issn = {1545-5955},
	doi = {10.1109/TASE.2014.2376492},
	abstract = {The Cloud infrastructure and its extensive set of Internet-accessible resources has potential to provide significant benefits to robots and automation systems. We consider robots and automation systems that rely on data or code from a network to support their operation, i.e., where not all sensing, computation, and memory is integrated into a standalone system. This survey is organized around four potential benefits of the Cloud: 1) Big Data: access to libraries of images, maps, trajectories, and descriptive data; 2) Cloud Computing: access to parallel grid computing on demand for statistical analysis, learning, and motion planning; 3) Collective Robot Learning: robots sharing trajectories, control policies, and outcomes; and 4) Human Computation: use of crowdsourcing to tap human skills for analyzing images and video, classification, learning, and error recovery. The Cloud can also improve robots and automation systems by providing access to: a) datasets, publications, models, benchmarks, and simulation tools; b) open competitions for designs and systems; and c) open-source software. This survey includes over 150 references on results and open challenges. A website with new developments and updates is available at: http://goldberg.berkeley.edu/cloud-robotics/.},
	number = {2},
	journal = {IEEE Transactions on Automation Science and Engineering},
	author = {Kehoe, B. and Patil, S. and Abbeel, P. and Goldberg, K.},
	month = apr,
	year = {2015},
	keywords = {Automation, automation system, Big data, Big Data, cloud automation, cloud computing, Cloud computing, cloud robotics, collective robot learning, Computational modeling, control engineering computing, crowdsourcing, grid computing, groupware, human computation, learning (artificial intelligence), open source, outsourcing, parallel grid computing, parallel processing, Robot kinematics, Robot sensing systems, robots, robots sharing trajectory},
	pages = {398--409},
	file = {IEEE Xplore Abstract Record:/Users/marc/Zotero/storage/IAZNEIXC/7006734.html:text/html;IEEE Xplore Full Text PDF:/Users/marc/Zotero/storage/ANRUHSMQ/Kehoe et al. - 2015 - A Survey of Research on Cloud Robotics and Automat.pdf:application/pdf}
}

@article{pas_grasp_2017,
	title = {Grasp {Pose} {Detection} in {Point} {Clouds}},
	url = {http://arxiv.org/abs/1706.09911},
	abstract = {Recently, a number of grasp detection methods have been proposed that can be used to localize robotic grasp conﬁgurations directly from sensor data without estimating object pose. The underlying idea is to treat grasp perception analogously to object detection in computer vision. These methods take as input a noisy and partially occluded RGBD image or point cloud and produce as output pose estimates of viable grasps, without assuming a known CAD model of the object. Although these methods generalize grasp knowledge to new objects well, they have not yet been demonstrated to be reliable enough for wide use. Many grasp detection methods achieve grasp success rates (grasp successes as a fraction of the total number of grasp attempts) between 75\% and 95\% for novel objects presented in isolation or in light clutter. Not only are these success rates too low for practical grasping applications, but the light clutter scenarios that are evaluated often do not reﬂect the realities of real world grasping. This paper proposes a number of innovations that together result in a signiﬁcant improvement in grasp detection performance. The speciﬁc improvement in performance due to each of our contributions is quantitatively measured either in simulation or on robotic hardware. Ultimately, we report a series of robotic experiments that average a 93\% end-to-end grasp success rate for novel objects presented in dense clutter.},
	language = {en},
	urldate = {2019-05-13},
	journal = {arXiv:1706.09911 [cs]},
	author = {Pas, Andreas ten and Gualtieri, Marcus and Saenko, Kate and Platt, Robert},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.09911},
	keywords = {Computer Science - Robotics},
	annote = {Comment: arXiv admin note: text overlap with arXiv:1603.01564},
	file = {Pas et al. - 2017 - Grasp Pose Detection in Point Clouds.pdf:/Users/marc/Zotero/storage/MHFMCVJZ/Pas et al. - 2017 - Grasp Pose Detection in Point Clouds.pdf:application/pdf}
}

@article{chitta_perception_2012,
	title = {Perception, planning, and execution for mobile manipulation in unstructured environments},
	volume = {19},
	number = {2},
	journal = {IEEE Robotics and Automation Magazine, Special Issue on Mobile Manipulation},
	author = {Chitta, Sachin and Jones, E. Gil and Ciocarlie, Matei and Hsiao, Kaijen},
	year = {2012},
	pages = {58--71}
}

@article{1Visual_learning_2005,
	author={M. {Lopes} and J. {Santos-Victor}},
	journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
	title={Visual learning by imitation with motor representations},
	year={2005},
	volume={35},
	number={3},
	pages={438-449},
	keywords={gesture recognition;image representation;psychology;robot vision;image sequences;robot kinematics;learning (artificial intelligence);humanoid robots;visual learning;motor representation;program level visual imitation;action-level imitation;viewpoint transformation;VPT;visuo-motor map;VMM map;motor data;motor information;gesture recognition system;iconic image representation;kinematic model;video sequence;visuomotor neuron;F5 area;macaque brain;anthropomorphic robot;visuomotor coordination;Robot kinematics;Cognitive robotics;Humans;Robot sensing systems;Image recognition;Mirrors;Focusing;Image representation;Video sequences;Neurons;Anthropomorphic robots;imitation;learning;visuomotor coordination;Algorithms;Artificial Intelligence;Biomimetics;Bionics;Computer Simulation;Hand;Hand Strength;Humans;Image Interpretation, Computer-Assisted;Models, Biological;Motor Skills;Movement;Pattern Recognition, Automated;Robotics;Vision},
	doi={10.1109/TSMCB.2005.846654},
	ISSN={1083-4419},
	month={June}
}

@article{2Teleoperation:2014,
  title={Teleoperation of humanoid baxter robot using haptic feedback},
  author={Ju, Zhangfeng and Yang, Chenguang and Li, Zhijun and Cheng, Long and Ma, Hongbin},
  booktitle={2014 International Conference on Multisensor Fusion and Information Integration for Intelligent Systems (MFI)},
  pages={1--6},
  year={2014},
  organization={IEEE}
}

@article{3Robot_learning_2012,
author = {George Konidaris and Scott Kuindersma and Roderic Grupen and Andrew Barto},
title ={Robot learning from demonstration by constructing skill trees},
journal = {The International Journal of Robotics Research},
volume = {31},
number = {3},
pages = {360-375},
year = {2012},
doi = {10.1177/0278364911428653},
URL = { 
        https://doi.org/10.1177/0278364911428653
}, eprint = { 
        https://doi.org/10.1177/0278364911428653
}
,abstract = { We describe CST, an online algorithm for constructing skill trees from demonstration trajectories. CST segments a demonstration trajectory into a chain of component skills, where each skill has a goal and is assigned a suitable abstraction from an abstraction library. These properties permit skills to be improved efficiently using a policy learning algorithm. Chains from multiple demonstration trajectories are merged into a skill tree. We show that CST can be used to acquire skills from human demonstration in a dynamic continuous domain, and from both expert demonstration and learned control sequences on the uBot-5 mobile manipulator. }
}

@article{4Imitation_and_Reinforcement_Learning_2010,
	author={J. {Kober} and J. {Peters}},
	journal={IEEE Robotics Automation Magazine},
	title={Imitation and Reinforcement Learning},
	year={2010},
	volume={17},
	number={2},
	pages={55-62},
	keywords={discrete systems;learning (artificial intelligence);regression analysis;robots;imitation learning;reinforcement learning;motor primitive;whole arm manipulator;industrial robots;discrete dynamical system;weighted regression;ball-in-a-cup;ball paddling;Humans;Learning systems;Robot programming;Service robots;Intelligent robots;Manufacturing;Anthropomorphism;Stability;Planar motors;Legged locomotion},
	doi={10.1109/MRA.2010.936952},
	ISSN={1070-9932},
	month={June}
}


@article{5Robotic_grasping_and_contact,
	author={A. {Bicchi} and V. {Kumar}},
	booktitle={Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065)},
	title={Robotic grasping and contact: a review},
	year={2000},
	volume={1},
	number={},
	pages={348-353 vol.1},
	keywords={manipulator dynamics;force control;reviews;robotic grasping;contact models;manipulators;grippers;dynamics;force control;Grasping;Fingers;Service robots;Grippers;Fixtures;Humans;Friction;Wrist;Haptic interfaces;Actuators},
	doi={10.1109/ROBOT.2000.844081},
	ISSN={1050-4729},
	month={April}
}

@article{6bo2013unsupervised,
  title={Unsupervised feature learning for RGB-D based object recognition},
  author={Bo, Liefeng and Ren, Xiaofeng and Fox, Dieter},
  booktitle={Experimental Robotics},
  pages={387--402},
  year={2013},
  organization={Springer}
}

@inproceedings{7krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}


@incollection{laumond_tutorial_2017,
	address = {Cham},
	title = {A {Tutorial} on {Newton} {Methods} for {Constrained} {Trajectory} {Optimization} and {Relations} to {SLAM}, {Gaussian} {Process} {Smoothing}, {Optimal} {Control}, and {Probabilistic} {Inference}},
	volume = {117},
	isbn = {978-3-319-51546-5 978-3-319-51547-2},
	url = {http://link.springer.com/10.1007/978-3-319-51547-2_15},
	abstract = {Many state-of-the-art approaches to trajectory optimization and optimal control are intimately related to standard Newton methods. For researchers that work in the intersections of machine learning, robotics, control, and optimization, such relations are highly relevant but sometimes hard to see across disciplines, due also to the different notations and conventions used in the disciplines. The aim of this tutorial is to introduce to constrained trajectory optimization in a manner that allows us to establish these relations. We consider a basic but general formalization of the problem and discuss the structure of Newton steps in this setting. The computation of Newton steps can then be related to dynamic programming, establishing relations to DDP, iLQG, and AICO. We can also clarify how inverting a banded symmetric matrix is related to dynamic programming as well as message passing in Markov chains and factor graphs. Further, for a machine learner, path optimization and Gaussian Processes seem intuitively related problems. We establish such a relation and show how to solve a Gaussian Process-regularized path optimization problem efﬁciently. Further topics include how to derive an optimal controller around the path, model predictive control in constrained k-order control processes, and the pullback metric interpretation of the Gauss-Newton approximation.},
	language = {en},
	urldate = {2019-05-13},
	booktitle = {Geometric and {Numerical} {Foundations} of {Movements}},
	publisher = {Springer International Publishing},
	author = {Toussaint, Marc},
	editor = {Laumond, Jean-Paul and Mansard, Nicolas and Lasserre, Jean-Bernard},
	year = {2017},
	doi = {10.1007/978-3-319-51547-2_15},
	pages = {361--392},
	file = {Toussaint - 2017 - A Tutorial on Newton Methods for Constrained Traje.pdf:/Users/marc/Zotero/storage/854XTY74/Toussaint - 2017 - A Tutorial on Newton Methods for Constrained Traje.pdf:application/pdf}
}

@article{18_erhan2014scalable,
  title={Scalable object detection using deep neural networks},
  author={Erhan, Dumitru and Szegedy, Christian and Toshev, Alexander and Anguelov, Dragomir},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2147--2154},
  year={2014}
}

@article{8_lai2011large,
  title={A large-scale hierarchical multi-view rgb-d object dataset},
  author={Lai, Kevin and Bo, Liefeng and Ren, Xiaofeng and Fox, Dieter},
  booktitle={2011 IEEE international conference on robotics and automation},
  pages={1817--1824},
  year={2011},
  organization={IEEE}
}

@article{9_lai2012detection,
  title={Detection-based object labeling in 3d scenes},
  author={Lai, Kevin and Bo, Liefeng and Ren, Xiaofeng and Fox, Dieter},
  booktitle={2012 IEEE International Conference on Robotics and Automation},
  pages={1330--1337},
  year={2012},
  organization={IEEE}
}

@article{11_du2011rgb,
  title={RGB-D Mapping: Using Depth Cameras for Dense 3D Modeling of Indoor Environments},
  author={Du, H and Henry, P and Ren, X and Cheng, M and Goldman, DB and Seitz, SM and Fox, D},
  booktitle={Proceedings of the 13th international conference on Ubiquitous computing, Beijing, China},
  year={2011}
}

@article{13_saxena2008robotic,
  title={Robotic grasping of novel objects using vision},
  author={Saxena, Ashutosh and Driemeyer, Justin and Ng, Andrew Y},
  journal={The International Journal of Robotics Research},
  volume={27},
  number={2},
  pages={157--173},
  year={2008},
  publisher={Sage Publications Sage UK: London, England}
}

@article{14_rao2010grasping,
  title={Grasping novel objects with depth segmentation},
  author={Rao, Deepak and Le, Quoc V and Phoka, Thanathorn and Quigley, Morgan and Sudsang, Attawith and Ng, Andrew Y},
  booktitle={2010 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={2578--2585},
  year={2010},
  organization={IEEE}
}

@article{27_mahler2017dex,
  title={Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics},
  author={Mahler, Jeffrey and Liang, Jacky and Niyaz, Sherdil and Laskey, Michael and Doan, Richard and Liu, Xinyu and Ojea, Juan Aparicio and Goldberg, Ken},
  journal={arXiv preprint arXiv:1703.09312},
  year={2017}
}

@article{19_redmon2015real,
  title={Real-time grasp detection using convolutional neural networks},
  author={Redmon, Joseph and Angelova, Anelia},
  booktitle={2015 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={1316--1322},
  year={2015},
  organization={IEEE}
}




